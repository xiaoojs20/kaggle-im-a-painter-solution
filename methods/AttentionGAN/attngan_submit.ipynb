{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttentionGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.networks import define_G, define_D\n",
    "import os\n",
    "from options.test_options import TestOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import save_images\n",
    "from util import html\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    print('Warning: wandb package cannot be found. The option \"--use_wandb\" will result in error.')\n",
    "    \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_to_tensor(folder_path, n=100):\n",
    "    \"\"\"\n",
    "    从文件夹中读取前 n 张 JPG 图像，整理成 [n, 3, h, w] 的 Tensor。\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): 图片文件夹路径。\n",
    "        n (int): 读取的图片数量。\n",
    "        image_size (tuple): 图像调整大小后的尺寸 (h, w)。\n",
    "        normalize (bool): 是否归一化到 [0, 1]，默认为 False 保持 [0, 255]。\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: 形状为 [n, 3, h, w] 的 Tensor。\n",
    "    \"\"\"\n",
    "    # 获取文件夹中的 JPG 文件\n",
    "    jpg_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    # 确保读取的数量不超过文件数量\n",
    "    n = min(n, len(jpg_files))\n",
    "    \n",
    "    # 定义图像预处理\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),  # 调整大小\n",
    "        transforms.ToTensor(),          # 转为张量\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 归一化到 [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # 初始化一个列表存储图像张量\n",
    "    tensors = []\n",
    "    \n",
    "    for i, file in enumerate(jpg_files[:n]):\n",
    "        # 打开图像\n",
    "        img_path = os.path.join(folder_path, file)\n",
    "        img = Image.open(img_path).convert('RGB')  # 转换为 RGB\n",
    "        \n",
    "        # 应用预处理并添加到列表\n",
    "        tensor = transform(img)\n",
    "        tensors.append(tensor)\n",
    "    \n",
    "    # 将所有图像堆叠成一个张量 [n, 3, h, w]\n",
    "    final_tensor = torch.stack(tensors, dim=0)\n",
    "    return final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')  # 打开图片并转换为 RGB\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)  # 返回图片张量和文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def batch_inference(image_folder, output_folder, model, batch_size=1, one_only=False, save=False, n=100, suffix=\"\", device='cuda'):\n",
    "    # 定义预处理\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),  # 调整大小\n",
    "        transforms.ToTensor(),          # 转为张量\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 归一化到 [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    # 加载数据集\n",
    "    dataset = ImageFolderDataset(image_folder, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 确保输出文件夹存在\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 推理\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    img_index = 1  # 图片序号\n",
    "    output_list = []\n",
    "    # pil_to_tensor = transforms.ToTensor()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, filenames = batch\n",
    "            inputs = inputs.to(device)  # 将输入移动到设备\n",
    "            outputs = model(inputs)  # 推理\n",
    "            # print(outputs.shape)\n",
    "            # print(type(outputs))\n",
    "            outputs = outputs[0]  # 提取出实际的tensor\n",
    "            \n",
    "            outputs = (outputs + 1) / 2.0  # 反归一化到 [0, 1]\n",
    "            \n",
    "\n",
    "            # 保存结果\n",
    "            for i in range(outputs.size(0)):\n",
    "                im = transforms.ToPILImage()(outputs[i].cpu())  # 转为 PIL 图像\n",
    "                # im = Image.fromarray(output_image)  # 转为 PIL 图像\n",
    "                if save:\n",
    "                    im.save(os.path.join(output_folder, f\"fake_epoch_{suffix}.jpg\"))  # 按序号保存图片\n",
    "                    print(f\"Saved: fake_epoch_{suffix}.jpg\")\n",
    "                tensor = transform(im)  # 转为Tensor\n",
    "                output_list.append(tensor)\n",
    "                # output_list.append(im)\n",
    "                img_index += 1\n",
    "            \n",
    "            if one_only:\n",
    "                break\n",
    "            if n != -1 and img_index > n:\n",
    "                break\n",
    "            \n",
    "    output_tensor = torch.stack(output_list, dim=0)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件夹路径\n",
    "image_folder = \"./datasets/photo2monet/trainA\"  # 输入图片文件夹\n",
    "output_folder = \"./datasets/photo2monet/fakeB_epochs\"  # 输出图片文件夹\n",
    "real_B_folder = \"./datasets/photo2monet/trainB\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with normal\n"
     ]
    }
   ],
   "source": [
    "model = define_G(input_nc=3, output_nc=3, ngf=64, netG='our', norm='instance', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[0])\n",
    "# every 5 epoch inference result pics [5, 10, ..., 400] -> gif\n",
    "# batch_size = 1  \n",
    "# for epoch in range(5, 401, 5):\n",
    "#     print(f\"epoch: {epoch}\")\n",
    "#     checkpoints = torch.load(f'./checkpoints/photo2monet_cut_md/{epoch}_net_G.pth', map_location='cuda')\n",
    "#     model.load_state_dict(checkpoints)\n",
    "#     batch_inference(image_folder, output_folder, model, batch_size=batch_size, one_only=True, suffix=epoch)\n",
    "    \n",
    "# checkpoints = torch.load('./checkpoints/photo2monet_CUT_md/latest_net_G.pth', map_location='cuda')\n",
    "# new_checkpoints = {f'module.{k}': v for k, v in checkpoints.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.conv1.weight', 'module.conv1.bias', 'module.conv2.weight', 'module.conv2.bias', 'module.conv3.weight', 'module.conv3.bias', 'module.resnet_blocks.0.conv1.weight', 'module.resnet_blocks.0.conv1.bias', 'module.resnet_blocks.0.conv2.weight', 'module.resnet_blocks.0.conv2.bias', 'module.resnet_blocks.1.conv1.weight', 'module.resnet_blocks.1.conv1.bias', 'module.resnet_blocks.1.conv2.weight', 'module.resnet_blocks.1.conv2.bias', 'module.resnet_blocks.2.conv1.weight', 'module.resnet_blocks.2.conv1.bias', 'module.resnet_blocks.2.conv2.weight', 'module.resnet_blocks.2.conv2.bias', 'module.resnet_blocks.3.conv1.weight', 'module.resnet_blocks.3.conv1.bias', 'module.resnet_blocks.3.conv2.weight', 'module.resnet_blocks.3.conv2.bias', 'module.resnet_blocks.4.conv1.weight', 'module.resnet_blocks.4.conv1.bias', 'module.resnet_blocks.4.conv2.weight', 'module.resnet_blocks.4.conv2.bias', 'module.resnet_blocks.5.conv1.weight', 'module.resnet_blocks.5.conv1.bias', 'module.resnet_blocks.5.conv2.weight', 'module.resnet_blocks.5.conv2.bias', 'module.resnet_blocks.6.conv1.weight', 'module.resnet_blocks.6.conv1.bias', 'module.resnet_blocks.6.conv2.weight', 'module.resnet_blocks.6.conv2.bias', 'module.resnet_blocks.7.conv1.weight', 'module.resnet_blocks.7.conv1.bias', 'module.resnet_blocks.7.conv2.weight', 'module.resnet_blocks.7.conv2.bias', 'module.resnet_blocks.8.conv1.weight', 'module.resnet_blocks.8.conv1.bias', 'module.resnet_blocks.8.conv2.weight', 'module.resnet_blocks.8.conv2.bias', 'module.deconv1_content.weight', 'module.deconv1_content.bias', 'module.deconv2_content.weight', 'module.deconv2_content.bias', 'module.deconv3_content.weight', 'module.deconv3_content.bias', 'module.deconv1_attention.weight', 'module.deconv1_attention.bias', 'module.deconv2_attention.weight', 'module.deconv2_attention.bias', 'module.deconv3_attention.weight', 'module.deconv3_attention.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of parameters in model:\", len(model.state_dict().keys()))\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of parameters in checkpoints:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mcheckpoints\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m      2\u001b[0m checkpoints\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoints' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters in checkpoints:\", len(checkpoints.keys()))\n",
    "checkpoints.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Epoch 5: FID = 2.4287\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 10\n",
      "Epoch 10: FID = 2.8925\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 15\n",
      "Epoch 15: FID = 3.2808\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 20\n",
      "Epoch 20: FID = 3.7209\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 25\n",
      "Epoch 25: FID = 3.4137\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 30\n",
      "Epoch 30: FID = 3.1290\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 35\n",
      "Epoch 35: FID = 3.3979\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 40\n",
      "Epoch 40: FID = 3.7765\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 45\n",
      "Epoch 45: FID = 3.9743\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 50\n",
      "Epoch 50: FID = 3.7304\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 55\n",
      "Epoch 55: FID = 3.4803\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 60\n",
      "Epoch 60: FID = 3.4595\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 65\n",
      "Epoch 65: FID = 3.6123\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 70\n",
      "Epoch 70: FID = 3.7204\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 75\n",
      "Epoch 75: FID = 3.6480\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 80\n",
      "Epoch 80: FID = 3.5405\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 85\n",
      "Epoch 85: FID = 3.8844\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 90\n",
      "Epoch 90: FID = 3.5719\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 95\n",
      "Epoch 95: FID = 4.2580\n",
      "Current CUDA memory allocated: 46.85 MB\n",
      "epoch: 100\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "sample_n = 1000\n",
    "\n",
    "real_B_tensor = load_images_to_tensor(real_B_folder, n=sample_n)\n",
    "real_B_tensor_unnorm= (real_B_tensor + 1).mul(127.5).clamp(0, 255).byte().to(device)  # [-1, 1] -> [0, 255]\n",
    "\n",
    "\n",
    "batch_size = 1  \n",
    "fid_list = []\n",
    "\n",
    "checkpoints_path = './checkpoints/photo2monet_attentiongan_epoch_200/' # pretrain_cut 300\n",
    "max_epoch = 160\n",
    "\n",
    "for epoch in range(5, max_epoch+1, 5):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    \n",
    "    \n",
    "    # load_filename = '%s_net_%s.pth' % (epoch, name)\n",
    "    load_path = f'{checkpoints_path}{epoch}_net_G_A.pth'\n",
    "    # print(f'loading the model from {load_path}')\n",
    "    checkpoints = torch.load(load_path, map_location='cuda')\n",
    "    \n",
    "    # 如果当前模型是 DataParallel 包装的，先还原回原始模型\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module\n",
    "    # 移除元数据（如果有）\n",
    "    if hasattr(checkpoints, '_metadata'):\n",
    "        del checkpoints._metadata\n",
    "\n",
    "    # 直接加载 state_dict（此时不再对 key 增加 'module.' 前缀）\n",
    "    model.load_state_dict(checkpoints)\n",
    "    model = model.cpu()\n",
    "                \n",
    "                \n",
    "    # checkpoints = torch.load(f'{checkpoints_path}{epoch}_net_G_A.pth', map_location='cuda')\n",
    "    # new_checkpoints = {f'module.{k}': v for k, v in checkpoints.items()}\n",
    "    # model.load_state_dict(new_checkpoints)\n",
    "    \n",
    "    fake_B_tensor = batch_inference(image_folder, output_folder, model, batch_size=1, one_only=False, save=False, n=sample_n, suffix=\"\", device=device)\n",
    "    fake_B_tensor_unnorm = (fake_B_tensor + 1).mul(127.5).clamp(0, 255).byte().to(device)\n",
    "    \n",
    "    # realB, fakeB ,值域检查\n",
    "    # print(real_B_tensor_unnorm.min(), real_B_tensor_unnorm.max())\n",
    "    # print(fake_B_tensor_unnorm.min(), fake_B_tensor_unnorm.max())\n",
    "    \n",
    "    fid = FrechetInceptionDistance(feature=64).to(device)        \n",
    "    # sample 1000 realB and gen fakeB, calculate fid score\n",
    "    # -> int8/\n",
    "    fid.update(real_B_tensor_unnorm, real=True)\n",
    "    fid.update(fake_B_tensor_unnorm, real=False)\n",
    "    \n",
    "    current_fid = fid.compute()\n",
    "    fid_list.append(current_fid)\n",
    "    print(f'Epoch {epoch}: FID = {current_fid:.4f}')\n",
    "    \n",
    "    # 清除显存\n",
    "    torch.cuda.empty_cache()\n",
    "    del fake_B_tensor\n",
    "    del fake_B_tensor_unnorm\n",
    "    del fid\n",
    "    del checkpoints\n",
    "    # del model\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fid_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot fid_list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# fid_list cuda->cpu\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m fid_list_cpu \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfid_list\u001b[49m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# fid_list_cpu = np.random.rand(max_epoch//5)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m smoothed_fid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconvolve(fid_list_cpu, np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fid_list' is not defined"
     ]
    }
   ],
   "source": [
    "# plot fid_list\n",
    "# fid_list cuda->cpu\n",
    "fid_list_cpu = [item.cpu().numpy() for item in fid_list]\n",
    "# fid_list_cpu = np.random.rand(max_epoch//5)\n",
    "smoothed_fid = np.convolve(fid_list_cpu, np.ones(5)/5, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# plt.style.use('seaborn-v0_8-deep')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "plt.plot(range(5, max_epoch+1, 5), fid_list_cpu, linestyle='-', label='FID')\n",
    "plt.plot(range(5, max_epoch+1, 5)[4:], smoothed_fid, linestyle='--', label='Smoothed FID', color='red')\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('FID', fontsize=14)\n",
    "plt.title('FID over Epochs', fontsize=16)\n",
    "plt.legend()\n",
    "plt.savefig(checkpoints_path + 'train_fid.png', format='png', dpi=300)\n",
    "plt.savefig(checkpoints_path + 'train_fid.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: [235, 280, 155, 260, 80, 350, 190, 390, 360, 220, 225, 270, 370, 330, 300, 295, 395, 400, 380, 315, 355, 375, 310, 175, 365, 385, 345, 245, 275, 325, 340, 320, 335, 265, 70, 215, 75, 290, 120, 230, 250, 305, 285, 150, 210, 180, 240, 145, 200, 165, 85, 135, 205, 140, 90, 100, 130, 105, 115, 255, 160, 40, 65, 170, 60, 50, 95, 185, 15, 195, 125, 25, 30, 110, 10, 35, 20, 55, 45, 5]\n"
     ]
    }
   ],
   "source": [
    "# sort best epoch of fid\n",
    "sorted_indices = sorted(range(len(fid_list_cpu)), key=lambda i: fid_list_cpu[i])\n",
    "\n",
    "sorted_epochs = [5 + 5 * i for i in sorted_indices]\n",
    "print(f'Best Epoch: {sorted_epochs}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF 动图已保存到：./datasets/photo2monet/fakeB_epochs/output.gif\n"
     ]
    }
   ],
   "source": [
    "def create_gif_from_jpgs(folder_path, output_gif_path, duration=500):\n",
    "    \"\"\"\n",
    "    将指定文件夹内的 JPG 图片拼成 GIF 动图。\n",
    "\n",
    "    :param folder_path: 包含 JPG 图片的文件夹路径\n",
    "    :param output_gif_path: 生成的 GIF 文件路径\n",
    "    :param duration: 每帧的持续时间（毫秒）\n",
    "    \"\"\"\n",
    "    # 获取文件夹内所有 .jpg 文件，并按文件名排序\n",
    "    images = sorted(\n",
    "        [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n",
    "    )\n",
    "    \n",
    "    if not images:\n",
    "        print(\"文件夹中没有找到 JPG 图片！\")\n",
    "        return\n",
    "    \n",
    "    # 打开图片文件并转换为帧，隔一张画\n",
    "    # frames = [Image.open(img) for img in images]\n",
    "    frames = [Image.open(img) for img in images[::2]]\n",
    "    \n",
    "    # 保存为 GIF 动图\n",
    "    frames[0].save(\n",
    "        output_gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],  # 添加后续帧\n",
    "        duration=duration,        # 每帧持续时间\n",
    "        loop=0                    # 循环播放，0 表示无限循环\n",
    "    )\n",
    "    print(f\"GIF 动图已保存到：{output_gif_path}\")\n",
    "\n",
    "\n",
    "create_gif_from_jpgs(output_folder, os.path.join(output_folder, \"output.gif\"), duration=200)  # 设置帧间隔为30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
