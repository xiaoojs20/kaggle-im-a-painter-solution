# ğŸ¨ I'm Something of a Painter Myself â€” Monet Style Transfer
### Unified Implementation of CycleGAN Â· CUT Â· AttentionGAN

This repository contains a complete solution for the Kaggle competition  
**â€œIâ€™m Something of a Painter Myselfâ€**, which focuses on generating Monet-style
paintings from natural photos using *unpaired image-to-image translation*.

We reconstruct and unify three representative GAN models:

- **CycleGAN** (ICCV 2017)
- **CUT â€“ Contrastive Unpaired Translation** (ECCV 2020)
- **AttentionGAN** (WACV 2021)

---

## ğŸ† Competition Performance

Our team achieved a **final score of 36.94468** on the leaderboard, ranking:

# ğŸ‰ **4th out of 170 teams (Top 2%)**

This reflects strong model performance and stability across multiple architectures.

![Score](imgs/score.png)

> *(Score and ranking were recorded during project reporting; leaderboard may later change depending on competition timeline.)*

---

## ğŸ¤— HuggingFace Model Release

Our best-performing CUT generator (Photo â†’ Monet) has been fully open-sourced on HuggingFace:

[![HuggingFace Model](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-cut--monet--style--transfer-yellow.svg)](https://huggingface.co/xiaoojs20/cut-monet-style-transfer)

---

## ğŸš€ Overview

The objective is to train models that generate 7,000â€“10,000 Monet-style images  
(256Ã—256 RGB) for submission.

The dataset consists of:

- Monet paintings (`monet_jpg`, `monet_tfrec`)
- Natural photos (`photo_jpg`, `photo_tfrec`)

Evaluation uses **FID** and **MI-FID**, measuring the distance between the
distribution of generated images and real Monet images.

---

## ğŸ“‚ Project Structure
```
painter/
â”œâ”€â”€ data/                 # dataset placeholder; real data not included
â”œâ”€â”€ imgs/                 # images for documentation (optional)
â”œâ”€â”€ notebooks/            # Kaggle submission & analysis notebooks
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # unified CycleGAN / CUT / AttentionGAN models
â”‚   â”œâ”€â”€ datasets/         # dataset loading utilities
â”‚   â”œâ”€â”€ util/             # training utilities (logging, losses, etc.)
â”‚   â”œâ”€â”€ train_cyclegan.py
â”‚   â”œâ”€â”€ train_cut.py
â”‚   â”œâ”€â”€ train_attngan.py
â”‚   â”œâ”€â”€ test_cyclegan.py
â”‚   â”œâ”€â”€ test_cut.py
â”‚   â””â”€â”€ test_attngan.py
â”œâ”€â”€ README.md
â””â”€â”€ README_zh.md
```
---

## ğŸ§  Models

### **1ï¸âƒ£ CycleGAN**
CycleGAN performs unpaired translation using:

- Generatorâ€“Discriminator pairs  
- Cycle consistency loss  
- Least-squares GAN loss (LSGAN)

ğŸ’¡ *Observations during training*  
CycleGAN is sensitive to learning rate decay and requires batch size = 1,
consistent with official implementations.

---

### **2ï¸âƒ£ CUT (Contrastive Unpaired Translation)**

CUT replaces cycle-consistency with **PatchNCE contrastive loss**, allowing
stronger content preservation.

Key advantages:

- Faster convergence  
- Better structure retention  
- Works well on style transfer tasks with strong texture changes  

---

### **3ï¸âƒ£ AttentionGAN**

AttentionGAN enhances translation using:

- Spatial attention  
- Channel attention  
- Region-level style learning  

This often yields more aesthetically pleasing Monet strokes and textures.

---

## ğŸ“Š Results Summary

| Model         | Strengths                               | Weaknesses                          |
|---------------|------------------------------------------|--------------------------------------|
| **CycleGAN**  | Strong global style transfer             | Unstable training, sensitive to LR   |
| **CUT**       | Best content preservation                | Sometimes less stylized              |
| **AttentionGAN** | Finest local texture & color details | Training more computationally heavy  |

Final generated images demonstrate that all three methods can produce Monet-style transformations with different emphases on texture, structure, and artistic abstraction.

---

## ğŸ¯ CUT Training Analysis

To further optimize performance, we explored **four different training strategies**
for CUT. The corresponding FID scores and training curves are summarized below:

| Training Strategy         | FID Score |
|---------------------------|-----------|
| Standard Training         | ~1.8      |
| **Best-Restart**          | ~1.6 (Best) |
| High-Epoch Pretraining    | ~4.0      |
| Monet-Finetune            | ~1.8      |

Below are the training curves for each strategy:

### ğŸ“Œ Standard Training

<!-- ![CUT Standard Training](imgs/cut_standard.png) -->
<p align="center">
  <img src="imgs/cut_standard.png" alt="CUT Standard Training" width="60%">
</p>

### â­ Best-Restart (Best Performing Strategy)
This strategy restarts training from the best-performing checkpoint and leads to
the lowest FID (~1.6), showing improved stability and convergence.

<!-- ![CUT Best-Restart](imgs/cut_best_restart.png) -->
<p align="center">
  <img src="imgs/cut_best_restart.png" alt="CUT Standard Training" width="60%">
</p>

### ğŸ” High-Epoch Pretraining
Pretraining for many epochs before fine-tuning leads to weaker results (~4.0),
indicating potential overfitting or style drift.

<!-- ![CUT High Epoch Pretraining](imgs/cut_pretrain.png) -->
<p align="center">
  <img src="imgs/cut_pretrain.png" alt="CUT Standard Training" width="60%">
</p>

### ğŸ¨ Monet-Finetune
Fine-tuning specifically on Monet paintings restores performance to ~1.8 and improves style fidelity.

<!-- ![CUT Monet Finetune](imgs/cut_monet_finetune.png) -->
<p align="center">
  <img src="imgs/cut_monet_finetune.png" alt="CUT Standard Training" width="60%">
</p>


These results highlight the importance of checkpoint selection and staged
training when using CUT for artistic style transfer tasks.

---

## ğŸ› ï¸ How to Train

### **CycleGAN**
```bash
python src/train_cyclegan.py
```
### **CUT**
```bash
python src/train_cut.py
```
### **AttentionGAN**
```bash
python src/train_attngan.py
```

### ğŸ“Œ Notes  
- The training shell scripts under `scripts/` serve as usage examples for all
  models.  
- Real datasets and checkpoints are **excluded** due to storage limitations.  
---

ğŸ™ Acknowledgements

This project builds on the outstanding work of these open-source implementations:
- CycleGAN â€“ Zhu et al., ICCV 2017
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
- CUT â€“ Park et al., ECCV 2020
https://github.com/taesungp/contrastive-unpaired-translation
- AttentionGAN â€“ Tang et al., WACV 2021
https://github.com/Ha0Tang/AttentionGAN